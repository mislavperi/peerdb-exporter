package peerdb

import (
	"context"
	"fmt"
	"log"
	"strconv"
	"time"

	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/prometheus/client_golang/prometheus"
)

type PeerDBExporter struct {
	db                 *pgxpool.Pool
	lastReportedTotals map[string]int64

	replicationLag      *prometheus.GaugeVec
	rowsSynced          *prometheus.CounterVec
	rowsSynced24Hours   *prometheus.GaugeVec
	flowErrors          *prometheus.CounterVec
	alerts              *prometheus.CounterVec
	flows               *prometheus.GaugeVec
	peers               *prometheus.GaugeVec
	syncThroughput      *prometheus.GaugeVec
	lastSyncTime        *prometheus.GaugeVec
	slotSize            *prometheus.GaugeVec
	batchProcessingTime *prometheus.GaugeVec
	qrepPartitionStatus *prometheus.GaugeVec
	pendingRows         *prometheus.GaugeVec
}

func NewPeerDBExporter(pgpool *pgxpool.Pool) *PeerDBExporter {
	exporter := &PeerDBExporter{
		db:                 pgpool,
		lastReportedTotals: make(map[string]int64),
		replicationLag: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_replication_lag_seconds",
				Help: "Current replication lag in seconds based on LSN difference",
			},
			[]string{"peer_name", "flow_name"},
		),
		rowsSynced: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "peerdb_rows_synced_total",
				Help: "Total number of rows synced per table",
			},
			[]string{"batch_id", "flow_name", "table_name"},
		),
		rowsSynced24Hours: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_rows_synced_24_hours",
				Help: "Total number of rows synced per table in the last 24 hours",
			},
			[]string{"batch_id", "flow_name", "table_name"},
		),
		alerts: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "peerdb_alerts",
				Help: "Total number of alerts generated by PeerDB",
			},
			[]string{"alert_key", "alert_level", "alert_message", "created_timestamp"},
		),
		flowErrors: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "peerdb_flow_errors_total",
				Help: "Total number of flow errors",
			},
			[]string{"flow_name", "error_type", "error_timestamp"},
		),
		flows: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_flows_details",
				Help: "List of flows and their status",
			},
			[]string{"name", "source_peer", "destination_peer", "flow_status", "status"},
		),
		peers: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_peers_status",
				Help: "List of peers and their status",
			},
			[]string{"name", "type"},
		),
		syncThroughput: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_sync_throughput_rows_per_second",
				Help: "Current sync throughput in rows per second",
			},
			[]string{"batch_id", "peer_name", "flow_name"},
		),
		lastSyncTime: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_last_sync_timestamp_seconds",
				Help: "Timestamp of last successful sync",
			},
			[]string{"peer_name", "flow_name"},
		),
		slotSize: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_replication_slot_size_bytes",
				Help: "Size of replication slot indicating lag",
			},
			[]string{"peer_name"},
		),
		batchProcessingTime: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_batch_processing_time_seconds",
				Help: "Time taken to process batches",
			},
			[]string{"batch_id", "flow_name"},
		),
		qrepPartitionStatus: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_qrep_partition_status",
				Help: "Status of query replication partitions (1=completed, 0=pending/failed)",
			},
			[]string{"peer_name", "flow_name", "partition_id"},
		),
		pendingRows: prometheus.NewGaugeVec(
			prometheus.GaugeOpts{
				Name: "peerdb_pending_rows_total",
				Help: "Number of rows pending sync per table",
			},
			[]string{"peer_name", "flow_name", "table_name"},
		),
	}

	prometheus.MustRegister(
		exporter.replicationLag,
		exporter.rowsSynced,
		exporter.rowsSynced24Hours,
		exporter.flowErrors,
		exporter.alerts,
		exporter.flows,
		exporter.peers,
		exporter.syncThroughput,
		exporter.lastSyncTime,
		exporter.slotSize,
		exporter.batchProcessingTime,
		exporter.qrepPartitionStatus,
		exporter.pendingRows,
	)

	return exporter
}

func (e *PeerDBExporter) collectReplicationMetrics() error {
	query := `
		SELECT 
			flow_name,
			latest_lsn_at_source,
			latest_lsn_at_target
		FROM peerdb_stats.cdc_flows
		WHERE flow_name IS NOT NULL
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		return fmt.Errorf("failed to query replication metrics from cdc_flows: %w", err)
	}
	defer rows.Close()

	for rows.Next() {
		var flowName string
		var sourceLSN, targetLSN *int64

		err := rows.Scan(&flowName, &sourceLSN, &targetLSN)
		if err != nil {
			log.Printf("Error scanning replication row: %v", err)
			continue
		}

		// This is only an approximate
		if sourceLSN != nil && targetLSN != nil {
			lag := float64(*sourceLSN - *targetLSN)
			e.replicationLag.WithLabelValues("", flowName).Set(lag)
		}
	}

	return rows.Err()
}

func (e *PeerDBExporter) collectBatchMetrics() error {
	query := `
SELECT 
    batch_id,
    flow_name,
    rows_in_batch,
    EXTRACT(EPOCH FROM (end_time - start_time)) as processing_time_seconds,
    CASE 
        WHEN end_time IS NOT NULL THEN rows_in_batch / GREATEST(EXTRACT(EPOCH FROM (end_time - start_time)), 1)
        ELSE 0 
    END as throughput
FROM peerdb_stats.cdc_batches
WHERE end_time IS NOT NULL
ORDER BY end_time DESC
LIMIT 500;
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		return fmt.Errorf("failed to query batch metrics: %w", err)
	}
	defer rows.Close()

	for rows.Next() {
		var flowName string
		var numRows, batchID int64
		var processingTime, throughput float64

		err := rows.Scan(&batchID, &flowName, &numRows, &processingTime, &throughput)
		if err != nil {
			log.Printf("Error scanning batch row: %v", err)
			continue
		}

		e.batchProcessingTime.WithLabelValues(strconv.Itoa(int(batchID)), flowName).Set(processingTime)
		e.syncThroughput.WithLabelValues(strconv.Itoa(int(batchID)), flowName).Set(throughput)
	}

	return rows.Err()
}

func (e *PeerDBExporter) collectTableMetrics() error {
	query := `
SELECT
		cb.batch_id,
    cb.flow_name,
    cbt.destination_table_name,
    COALESCE(SUM(cbt.num_rows), 0) AS total_rows
FROM peerdb_stats.cdc_batch_table cbt
JOIN (
    SELECT batch_id, flow_name, start_time
    FROM (
        SELECT
            batch_id,
            flow_name,
            start_time,
            ROW_NUMBER() OVER (PARTITION BY batch_id ORDER BY start_time DESC) AS rn
        FROM peerdb_stats.cdc_batches
    ) t
    WHERE rn = 1
) cb ON cb.batch_id = cbt.batch_id
GROUP BY cb.batch_id, cb.flow_name, cbt.destination_table_name;
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		return fmt.Errorf("failed to query table metrics: %w", err)
	}
	defer rows.Close()

	for rows.Next() {
		var batchID, flowName, tableName string
		var totalRows int64

		err := rows.Scan(&batchID, &flowName, &tableName, &totalRows)
		if err != nil {
			log.Printf("Error scanning table metrics row: %v", err)
			continue
		}

		key := fmt.Sprintf("%s|%s|%s", batchID, flowName, tableName)
		lastTotal := e.lastReportedTotals[key]
		delta := totalRows - lastTotal

		if delta > 0 {
			e.rowsSynced.WithLabelValues(batchID, flowName, tableName).Add(float64(delta))
			e.lastReportedTotals[key] = totalRows
		}
	}

	return rows.Err()
}

func (e *PeerDBExporter) collectSyncedRows24Hours() error {
	query := `
SELECT
		cb.batch_id,
    cb.flow_name,
    cbt.destination_table_name,
    COALESCE(SUM(cbt.num_rows), 0) AS total_rows
FROM peerdb_stats.cdc_batch_table cbt
JOIN (
    SELECT batch_id, flow_name, start_time
    FROM (
        SELECT
            batch_id,
            flow_name,
            start_time,
            ROW_NUMBER() OVER (PARTITION BY batch_id ORDER BY start_time DESC) AS rn
        FROM peerdb_stats.cdc_batches
    ) t
    WHERE rn = 1
) cb ON cb.batch_id = cbt.batch_id
WHERE cb.start_time > NOW() - INTERVAL '24 hours'
GROUP BY cb.batch_id, cb.flow_name, cbt.destination_table_name;
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		return fmt.Errorf("failed to query table metrics: %w", err)
	}
	defer rows.Close()

	for rows.Next() {
		var batchID, flowName, tableName string
		var totalRows int64

		err := rows.Scan(&batchID, &flowName, &tableName, &totalRows)
		if err != nil {
			log.Printf("Error scanning table metrics row: %v", err)
			continue
		}

		e.rowsSynced24Hours.WithLabelValues(batchID, flowName, tableName).Set(float64(totalRows))
	}

	return rows.Err()
}

func (e *PeerDBExporter) collectSlotSizeMetrics() error {
	// Use peer_slot_size for replication slot lag
	query := `
		SELECT 
			peer_name,
			slot_size,
			updated_at
		FROM peerdb_stats.peer_slot_size
		ORDER BY updated_at DESC
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		return fmt.Errorf("failed to query slot size metrics: %w", err)
	}
	defer rows.Close()

	for rows.Next() {
		var peerName string
		var slotSize *int64
		var updatedAt time.Time

		err := rows.Scan(&peerName, &slotSize, &updatedAt)
		if err != nil {
			log.Printf("Error scanning slot size row: %v", err)
			continue
		}

		if slotSize != nil {
			e.slotSize.WithLabelValues(peerName).Set(float64(*slotSize))
		}
	}

	return rows.Err()
}

func (e *PeerDBExporter) collectQRepMetrics() error {
	query := `
		SELECT 
			flow_name,
			partition_uuid,
			rows_in_partition,
			CASE WHEN end_time IS NOT NULL THEN 1 ELSE 0 END as completed_status
		FROM peerdb_stats.qrep_partitions
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		log.Printf("Warning: Could not query qrep metrics (may not have query replication flows): %v", err)
		return nil
	}
	defer rows.Close()

	for rows.Next() {
		var flowName, partitionID string
		var numRows int64
		var completedStatus float64

		err := rows.Scan(&flowName, &partitionID, &numRows, &completedStatus)
		if err != nil {
			log.Printf("Error scanning qrep partition row: %v", err)
			continue
		}

		e.qrepPartitionStatus.WithLabelValues(flowName, partitionID).Set(completedStatus)

		if completedStatus == 0 {
			e.pendingRows.WithLabelValues(flowName, "qrep_partition").Set(float64(numRows))
		}
	}

	return rows.Err()
}

func (e *PeerDBExporter) collectFlowErrors() error {
	query := `
		SELECT
			flow_name,
			error_message,
			error_type,
			error_timestamp
		FROM
			peerdb_stats.flow_errors
		WHERE ack IS NULL
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		log.Printf("Warning: Could not query flow errors: %v", err)
		return nil
	}
	defer rows.Close()

	for rows.Next() {
		var flowName, errorMessage, errorType string

		err := rows.Scan(&flowName, &errorMessage, &errorType)
		if err != nil {
			log.Printf("Error scanning for flow errors: %v", err)
			continue
		}

		e.flowErrors.WithLabelValues(flowName, errorMessage, errorType)
	}

	return rows.Err()
}

func (e *PeerDBExporter) collectAlerts() error {
	query := `
		SELECT
			alert_key,
			alert_level,
			alert_message,
			created_timestamp
		FROM
			peerdb_stats.alerts_v1
	`

	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		log.Printf("Warning: Could not load alert errors: %v", err)
		return nil
	}
	defer rows.Close()

	for rows.Next() {
		var alertKey, alertLevel, alertMessage, createdTimestamp string

		err := rows.Scan(&alertKey, &alertLevel, &alertMessage, &createdTimestamp)
		if err != nil {
			log.Printf("Error scanning for alerts: %v", err)
			return err
		}

		e.alerts.WithLabelValues(alertKey, alertLevel, alertMessage, createdTimestamp)
	}
	return rows.Err()
}

func (e *PeerDBExporter) collectPeers() error {
	query := `
		SELECT
			name,
			type,
		FROM
			public.peers
	`
	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		log.Printf("Warning: Could not load peers: %v", err)
		return nil
	}
	defer rows.Close()

	for rows.Next() {
		var name string
		var peerType int

		err := rows.Scan(&name, &peerType)
		if err != nil {
			log.Printf("Error scanning for alerts: %v", err)
			return err
		}

		e.peers.WithLabelValues(name, strconv.Itoa(peerType))
	}
	return rows.Err()
}

func (e *PeerDBExporter) collectFlows() error {
	query := `
		SELECT
			name,
			source_peer,
			destination_peer,
			flow_status,
			status
		FROM
			public.flows
	`
	rows, err := e.db.Query(context.Background(), query)
	if err != nil {
		log.Printf("Warning: Could not load flows: %v", err)
		return nil
	}
	defer rows.Close()

	for rows.Next() {
		var name, sourcePeer, destinationPeer string
		var flowStatus, status int

		err := rows.Scan(&name, &sourcePeer, &destinationPeer, &flowStatus, &status)
		if err != nil {
			log.Printf("Error scanning for alerts: %v", err)
			return err
		}

		e.flows.WithLabelValues(name, sourcePeer, destinationPeer, strconv.Itoa(flowStatus), strconv.Itoa(status))
	}
	return rows.Err()
}

func (e *PeerDBExporter) CollectMetrics() error {
	if err := e.collectReplicationMetrics(); err != nil {
		return fmt.Errorf("failed to collect replication metrics: %w", err)
	}

	if err := e.collectBatchMetrics(); err != nil {
		return fmt.Errorf("failed to collect batch metrics: %w", err)
	}

	if err := e.collectTableMetrics(); err != nil {
		return fmt.Errorf("failed to collect table metrics: %w", err)
	}

	if err := e.collectSyncedRows24Hours(); err != nil {
		return fmt.Errorf("failed to collect synced rows in 24 hours metrics: %w", err)
	}

	if err := e.collectSlotSizeMetrics(); err != nil {
		return fmt.Errorf("failed to collect slot size metrics: %w", err)
	}

	if err := e.collectQRepMetrics(); err != nil {
		log.Printf("Warning: Could not collect QRep metrics: %v", err)
	}

	if err := e.collectFlowErrors(); err != nil {
		log.Printf("Warning: Could not collect flow errors: %v", err)
	}

	if err := e.collectFlows(); err != nil {
		log.Printf("Warning: Could not collect flows: %v", err)
	}

	if err := e.collectAlerts(); err != nil {
		log.Printf("Warning: Could not collect alerts: %v", err)
	}

	if err := e.collectPeers(); err != nil {
		log.Printf("Warning: Could not collect peers: %v", err)
	}
	return nil
}

func (e *PeerDBExporter) StartMetricsCollection(interval time.Duration) {
	go func() {
		ticker := time.NewTicker(interval)
		defer ticker.Stop()

		if err := e.CollectMetrics(); err != nil {
			log.Printf("Error collecting initial metrics: %v", err)
		}

		for range ticker.C {
			if err := e.CollectMetrics(); err != nil {
				log.Printf("Error collecting metrics: %v", err)
			} else {
				log.Printf("Successfully collected metrics at %v", time.Now())
			}
		}
	}()
}
